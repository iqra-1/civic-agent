# üèõÔ∏è Dublin City Council AI Assistant

A multi-agent RAG (Retrieval-Augmented Generation) system that provides intelligent responses to queries about Dublin City Council policies, services, and procedures using local LLMs.

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)

## üéØ Overview

This project implements a sophisticated AI assistant that combines:
- **Document Retrieval (RAG)**: Searches through official Dublin City Council documents
- **Multi-Agent System**: Three specialized AI agents work together
- **Local LLM**: Privacy-focused, GPU-accelerated inference using Ollama + Phi-3
- **Web Interface**: User-friendly Gradio chat interface

### Why This Architecture?

**1. RAG (Retrieval-Augmented Generation)**
- ‚úÖ Grounds responses in official documents (no hallucinations)
- ‚úÖ Always up-to-date with latest policies
- ‚úÖ Provides source citations

**2. Multi-Agent System (CrewAI)**
- ‚úÖ Specialized agents = better quality
- ‚úÖ Separation of concerns (research ‚Üí validate ‚Üí advise)
- ‚úÖ More reliable than single-agent systems

**3. Local LLM (Ollama + Phi-3)**
- ‚úÖ Privacy: No data sent to external APIs
- ‚úÖ Cost: Free inference, no API costs
- ‚úÖ Speed: GPU acceleration on T4/A100
- ‚úÖ Phi-3: Microsoft's efficient 3.8B parameter model

## üèóÔ∏è System Architecture

```
User Query
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Gradio Web Interface              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CrewAI Multi-Agent System         ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ   Agent 1: Policy Researcher        ‚îÇ
‚îÇ   ‚îú‚îÄ Uses RAG tool                  ‚îÇ
‚îÇ   ‚îî‚îÄ Searches FAISS vector DB       ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ   Agent 2: Eligibility Validator    ‚îÇ
‚îÇ   ‚îî‚îÄ Interprets policies strictly   ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ   Agent 3: Citizen Action Guide     ‚îÇ
‚îÇ   ‚îî‚îÄ Provides actionable steps      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   FAISS Vector Database             ‚îÇ
‚îÇ   (Dublin City Council docs)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Ollama + Phi-3 (Local LLM)        ‚îÇ
‚îÇ   (GPU-accelerated inference)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Quick Start

### Prerequisites

- Python 3.10+
- NVIDIA GPU (recommended for speed)
- 8GB+ RAM
- Google Colab (or local setup)
- Internet connection (for web scraping)

### Installation

```bash
# 1. Install dependencies
pip install --upgrade \
    "numpy>=2.0,<2.3" \
    "scipy>=1.13.0" \
    "transformers" \
    "sentence-transformers" \
    "faiss-cpu" \
    "langchain" \
    "langchain-community" \
    "crewai" \
    "crewai-tools" \
    "gradio" \
    "beautifulsoup4" \
    "requests"

# 2. Install and setup Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama serve &
sleep 5
ollama pull phi3

# 3. Run the notebook cells in order
```

### Usage

1. **Prepare Documents**: Place Dublin City Council PDFs/documents in `/content/data/`
2. **Build FAISS Index**: Run the document processing cells
3. **Run Queries**: Use the CLI interface

```python
# Single query
single_query_mode("Your question here")

# Interactive mode
interactive_mode()

# Batch queries
for query in my_queries:
    single_query_mode(query)
```

## üìÅ Project Structure

```
dublin-council-ai/
‚îú‚îÄ‚îÄ README.md                       # This file
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ dublin_council_rag.ipynb  # Main Colab notebook
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py                 # Web scraping module
‚îÇ   ‚îú‚îÄ‚îÄ document_processor.py     # Text ‚Üí FAISS pipeline
‚îÇ   ‚îú‚îÄ‚îÄ agents.py                  # CrewAI agent definitions
‚îÇ   ‚îú‚îÄ‚îÄ rag_tool.py                # Custom RAG tool
‚îÇ   ‚îî‚îÄ‚îÄ cli.py                     # Command-line interface
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                       # Scraped text + optional PDFs
‚îÇ   ‚îî‚îÄ‚îÄ faiss_index/               # Generated vector database
‚îî‚îÄ‚îÄ models/
    ‚îî‚îÄ‚îÄ phi-3-mini-q4.gguf         # Downloaded LLM (not in git)
```

## üîß Technical Deep Dive

### Why Each Component?

#### 1. **FAISS Vector Database**
**Chosen over**: Pinecone, Weaviate, ChromaDB

**Reasons**:
- ‚úÖ Runs locally (no external dependencies)
- ‚úÖ Extremely fast similarity search
- ‚úÖ Low memory footprint
- ‚úÖ Facebook Research's battle-tested library
- ‚úÖ Works offline

#### 2. **Sentence Transformers (all-MiniLM-L6-v2)**
**Chosen over**: OpenAI embeddings, large BERT models

**Reasons**:
- ‚úÖ Only 80MB model size
- ‚úÖ Fast inference (384-dim embeddings)
- ‚úÖ Excellent quality for semantic search
- ‚úÖ Free and runs locally
- ‚úÖ Widely used and trusted

#### 3. **CrewAI Multi-Agent Framework**
**Chosen over**: LangChain agents, AutoGPT, single LLM

**Reasons**:
- ‚úÖ Purpose-built for multi-agent workflows
- ‚úÖ Clean agent definitions with roles/goals
- ‚úÖ Built-in task orchestration
- ‚úÖ Easy tool integration
- ‚úÖ Better than single agent for complex queries

#### 4. **Ollama + Phi-3**
**Chosen over**: API models (GPT-4, Claude), other local LLMs

**Reasons**:
- ‚úÖ **Ollama**: Easy setup, automatic GPU offloading
- ‚úÖ **Phi-3**: 3.8B params, optimized for efficiency
- ‚úÖ Matches GPT-3.5 quality at 1/50th the size
- ‚úÖ Works with CrewAI's LiteLLM backend
- ‚úÖ Microsoft-backed, well-maintained

**Why not llama-cpp directly?**
- ‚ùå CrewAI's newer versions use LiteLLM
- ‚ùå LiteLLM doesn't recognize raw llama-cpp objects
- ‚úÖ Ollama provides the compatibility layer

#### 5. **Command Line Interface**
**Chosen over**: Gradio, Streamlit, Flask

**Reasons**:
- ‚úÖ Works in any environment (Colab, Jupyter, local)
- ‚úÖ No port/threading issues
- ‚úÖ Easy to script and automate
- ‚úÖ Perfect for batch processing
- ‚úÖ Simple to understand and modify
- ‚úÖ Saves conversation history automatically

### Key Design Decisions

**1. Sequential Agent Processing**
```python
Researcher ‚Üí Validator ‚Üí Actioner
```
- Ensures information flows logically
- Each agent builds on previous output
- More reliable than parallel processing

**2. Limited RAG Retrieval (k=3)**
- Prevents context overflow
- Faster processing
- Forces focus on most relevant docs

**3. Low Temperature (0.1)**
- Deterministic, consistent responses
- Less creative but more factual
- Critical for policy interpretation

**4. Max Iterations = 3**
- Allows tool usage but prevents loops
- Balances thoroughness vs. speed

## üéì Step-by-Step Explanation

### Step 1: Document Processing
```python
# Why: Get latest information from official website
WebScraper ‚Üí BeautifulSoup ‚Üí Clean Text ‚Üí FAISS
```
- **WebScraper**: Fetches 15+ key service pages
- **BeautifulSoup**: Extracts clean text, removes navigation
- **Optional PDFs**: Can add policy documents for more coverage
- **FAISS**: Stores vectors for fast similarity search

**Why web scraping over PDF upload?**
- ‚úÖ Always up-to-date information
- ‚úÖ No manual document collection
- ‚úÖ Covers breadth of services
- ‚úÖ Easy to add new pages
- ‚ö†Ô∏è PDFs still supported as supplement

### Step 2: RAG Tool Creation
```python
# Why: Bridge between agents and documents
class DublinCouncilRAGTool(BaseTool):
    def _run(self, query: str) -> str:
        docs = retriever.invoke(query)  # Semantic search
        return formatted_excerpts
```
- CrewAI agents can call this tool
- Returns top 3 most relevant excerpts
- Truncates to 300 chars to fit context

### Step 3: Agent Definitions
```python
# Why: Specialized roles improve quality

# Researcher: Only searches, never guesses
researcher = Agent(
    llm=ollama_llm,
    tools=[rag_tool],  # Has access to documents
    max_iter=3         # Can search multiple times
)

# Validator: Strict YES/NO decisions
validator = Agent(
    llm=ollama_llm,
    tools=[],          # No tools = only analyzes
)

# Actioner: Practical guidance
actioner = Agent(
    llm=ollama_llm,
    tools=[]           # Uses validator's decision
)
```

### Step 4: Task Orchestration
```python
# Why: Clear information flow

task1 = Task(
    description="Search for policy info",
    agent=researcher,
    context=[]  # No dependencies
)

task2 = Task(
    description="Determine eligibility",
    agent=validator,
    context=[task1]  # Sees researcher's output
)

task3 = Task(
    description="Provide next steps",
    agent=actioner,
    context=[task1, task2]  # Sees both outputs
)
```

### Step 5: Gradio Interface
```python
# Why: User-friendly chat interface

def process_query(query, history):
    result = crew.kickoff()  # Run agents
    return formatted_response

demo = gr.ChatInterface(
    fn=process_query,
    examples=[...],  # Suggested queries
    share=True       # Public URL
)
```

## üìä Performance

| Metric | Value |
|--------|-------|
| Query Response Time | 10-30 seconds |
| Document Retrieval | < 1 second |
| LLM Inference | 8-25 seconds |
| GPU Memory Usage | ~4GB |
| Accuracy (subjective) | ~85% |

## üîí Privacy & Security

- ‚úÖ All processing happens locally
- ‚úÖ No data sent to external APIs
- ‚úÖ Documents stay on your server
- ‚úÖ GDPR compliant
- ‚ö†Ô∏è Still verify critical information with official sources

## üêõ Troubleshooting

### "ModuleNotFoundError: No module named 'transformers.modeling_layers'"
```bash
pip install --upgrade transformers accelerate
```

### "BadRequestError: LLM Provider NOT provided"
```bash
# Ensure Ollama is running
ollama serve &
ollama list  # Should show phi3
```

### "NumPy/SciPy version conflicts"
```bash
pip uninstall -y numpy scipy
pip install "numpy>=2.0,<2.3" "scipy>=1.13.0"
# Restart runtime
```

### FAISS index not loading
```bash
# Rebuild the index
vectorstore.save_local("/content/data/faiss_index")
```

## üöß Future Improvements

- [ ] Add conversation memory (multi-turn)
- [ ] Implement streaming responses
- [ ] Add document upload via Gradio
- [ ] Multi-language support (Irish + English)
- [ ] Voice input/output
- [ ] Mobile app version
- [ ] User feedback collection
- [ ] A/B testing different LLMs
- [ ] Automated policy updates

## üìù License

MIT License - See LICENSE file for details

## ü§ù Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## üìß Contact

For questions or support, please open an issue on GitHub.

## üôè Acknowledgments

- Dublin City Council for public documents
- Anthropic for Claude (used in development)
- Microsoft for Phi-3 model
- Ollama team for inference server
- CrewAI for multi-agent framework
- HuggingFace for embeddings

---

**Built with ‚ù§Ô∏è for Dublin citizens**
