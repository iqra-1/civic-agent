# ğŸ›ï¸ Dublin City Council AI Assistant

A multi-agent RAG (Retrieval-Augmented Generation) system that provides intelligent responses to queries about Dublin City Council policies, services, and procedures using local LLMs.

![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Status](https://img.shields.io/badge/status-active-success.svg)

## ğŸ¯ Overview

This project implements a sophisticated AI assistant that combines:
- **Document Retrieval (RAG)**: Searches through official Dublin City Council documents
- **Multi-Agent System**: Three specialized AI agents work together
- **Local LLM**: Privacy-focused, GPU-accelerated inference using Ollama + Phi-3
- **Web Interface**: User-friendly Gradio chat interface

### Why This Architecture?

**1. RAG (Retrieval-Augmented Generation)**
- âœ… Grounds responses in official documents (no hallucinations)
- âœ… Always up-to-date with latest policies
- âœ… Provides source citations

**2. Multi-Agent System (CrewAI)**
- âœ… Specialized agents = better quality
- âœ… Separation of concerns (research â†’ validate â†’ advise)
- âœ… More reliable than single-agent systems

**3. Local LLM (Ollama + Phi-3)**
- âœ… Privacy: No data sent to external APIs
- âœ… Cost: Free inference, no API costs
- âœ… Speed: GPU acceleration on T4/A100
- âœ… Phi-3: Microsoft's efficient 3.8B parameter model


[//]: # ("Comment")
[Comment test]::
[//]: # (This may be the most platform independent comment)
## ğŸ—ï¸ System Architecture

```
User Query
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CrewAI Multi-Agent System         â”‚
â”‚                                     â”‚
â”‚   Agent 1: Policy Researcher        â”‚
â”‚   â”œâ”€ Uses RAG tool                  â”‚
â”‚   â””â”€ Searches FAISS vector DB       â”‚
â”‚                                     â”‚
â”‚   Agent 2: Eligibility Validator    â”‚
â”‚   â””â”€ Interprets policies strictly   â”‚
â”‚                                     â”‚
â”‚   Agent 3: Citizen Action Guide     â”‚
â”‚   â””â”€ Provides actionable steps      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FAISS Vector Database             â”‚
â”‚   (Dublin City Council docs)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama + Phi-3 (Local LLM)        â”‚
â”‚   (GPU-accelerated inference)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- NVIDIA GPU (recommended for speed)
- 8GB+ RAM
- Google Colab (or local setup)
- Internet connection (for web scraping)

### Installation

```bash
# 1. Install dependencies
pip install --upgrade \
    "numpy>=2.0,<2.3" \
    "scipy>=1.13.0" \
    "transformers" \
    "sentence-transformers" \
    "faiss-cpu" \
    "langchain" \
    "langchain-community" \
    "crewai" \
    "crewai-tools" \
    "gradio" \
    "beautifulsoup4" \
    "requests"

# 2. Install and setup Ollama
curl -fsSL https://ollama.com/install.sh | sh
ollama serve &
sleep 5
ollama pull phi3

# 3. Run the notebook cells in order
```

### Usage

1. **Prepare Documents**: Place Dublin City Council PDFs/documents in `/content/data/`
2. **Build FAISS Index**: Run the document processing cells
3. **Run Queries**: Use the CLI interface

```python
# Single query
single_query_mode("Your question here")

# Interactive mode
interactive_mode()

# Batch queries
for query in my_queries:
    single_query_mode(query)
```

## ğŸ“ Project Structure

```
dublin-council-ai/
â”œâ”€â”€ README.md                       # This file
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ dublin_council_rag.ipynb  # Main Colab notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scraper.py                 # Web scraping module
â”‚   â”œâ”€â”€ document_processor.py     # Text â†’ FAISS pipeline
â”‚   â”œâ”€â”€ agents.py                  # CrewAI agent definitions
â”‚   â”œâ”€â”€ rag_tool.py                # Custom RAG tool
â”‚   â””â”€â”€ cli.py                     # Command-line interface
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                       # Scraped text + optional PDFs
â”‚   â””â”€â”€ faiss_index/               # Generated vector database
â””â”€â”€ models/
    â””â”€â”€ phi-3-mini-q4.gguf         # Downloaded LLM (not in git)
```

## ğŸ”§ Technical Deep Dive

### Why Each Component?

#### 1. **FAISS Vector Database**
**Chosen over**: Pinecone, Weaviate, ChromaDB

**Reasons**:
- âœ… Runs locally (no external dependencies)
- âœ… Extremely fast similarity search
- âœ… Low memory footprint
- âœ… Facebook Research's battle-tested library
- âœ… Works offline

#### 2. **Sentence Transformers (all-MiniLM-L6-v2)**
**Chosen over**: OpenAI embeddings, large BERT models

**Reasons**:
- âœ… Only 80MB model size
- âœ… Fast inference (384-dim embeddings)
- âœ… Excellent quality for semantic search
- âœ… Free and runs locally
- âœ… Widely used and trusted

#### 3. **CrewAI Multi-Agent Framework**
**Chosen over**: LangChain agents, AutoGPT, single LLM

**Reasons**:
- âœ… Purpose-built for multi-agent workflows
- âœ… Clean agent definitions with roles/goals
- âœ… Built-in task orchestration
- âœ… Easy tool integration
- âœ… Better than single agent for complex queries

#### 4. **Ollama + Phi-3**
**Chosen over**: API models (GPT-4, Claude), other local LLMs

**Reasons**:
- âœ… **Ollama**: Easy setup, automatic GPU offloading
- âœ… **Phi-3**: 3.8B params, optimized for efficiency
- âœ… Matches GPT-3.5 quality at 1/50th the size
- âœ… Works with CrewAI's LiteLLM backend
- âœ… Microsoft-backed, well-maintained

**Why not llama-cpp directly?**
- âŒ CrewAI's newer versions use LiteLLM
- âŒ LiteLLM doesn't recognize raw llama-cpp objects
- âœ… Ollama provides the compatibility layer

<!--  #### 5. **Command Line Interface**
[//]: <>  **Chosen over**: Gradio, Streamlit, Flask

[//]: <>  **Reasons**:
[//]: <>  - âœ… Works in any environment (Colab, Jupyter, local)
[//]: <>  - âœ… No port/threading issues
[//]: <>  - âœ… Easy to script and automate
[//]: <>  - âœ… Perfect for batch processing
[//]: <>  - âœ… Simple to understand and modify
[//]: <>  - âœ… Saves conversation history automatically-->

### Key Design Decisions

**1. Sequential Agent Processing**
```python
Researcher â†’ Validator â†’ Actioner
```
- Ensures information flows logically
- Each agent builds on previous output
- More reliable than parallel processing

**2. Limited RAG Retrieval (k=3)**
- Prevents context overflow
- Faster processing
- Forces focus on most relevant docs

**3. Low Temperature (0.1)**
- Deterministic, consistent responses
- Less creative but more factual
- Critical for policy interpretation

**4. Max Iterations = 3**
- Allows tool usage but prevents loops
- Balances thoroughness vs. speed

## ğŸ“ Step-by-Step Explanation

### Step 1: Document Processing
```python
# Why: Get latest information from official website
WebScraper â†’ BeautifulSoup â†’ Clean Text â†’ FAISS
```
- **WebScraper**: Fetches 15+ key service pages
- **BeautifulSoup**: Extracts clean text, removes navigation
- **Optional PDFs**: Can add policy documents for more coverage
- **FAISS**: Stores vectors for fast similarity search

<!-- **Why web scraping over PDF upload?**
[//]: <>  - âœ… Always up-to-date information
[//]: <>  - âœ… No manual document collection
[//]: <>  - âœ… Covers breadth of services
[//]: <>  - âœ… Easy to add new pages
[//]: <>  - âš ï¸ PDFs still supported as supplement-->

### Step 2: RAG Tool Creation
```python
# Why: Bridge between agents and documents
class DublinCouncilRAGTool(BaseTool):
    def _run(self, query: str) -> str:
        docs = retriever.invoke(query)  # Semantic search
        return formatted_excerpts
```
- CrewAI agents can call this tool
- Returns top 3 most relevant excerpts
- Truncates to 300 chars to fit context

### Step 3: Agent Definitions
```python
# Why: Specialized roles improve quality

# Researcher: Only searches, never guesses
researcher = Agent(
    llm=ollama_llm,
    tools=[rag_tool],  # Has access to documents
    max_iter=3         # Can search multiple times
)

# Validator: Strict YES/NO decisions
validator = Agent(
    llm=ollama_llm,
    tools=[],          # No tools = only analyzes
)

# Actioner: Practical guidance
actioner = Agent(
    llm=ollama_llm,
    tools=[]           # Uses validator's decision
)
```

### Step 4: Task Orchestration
```python
# Why: Clear information flow

task1 = Task(
    description="Search for policy info",
    agent=researcher,
    context=[]  # No dependencies
)

task2 = Task(
    description="Determine eligibility",
    agent=validator,
    context=[task1]  # Sees researcher's output
)

task3 = Task(
    description="Provide next steps",
    agent=actioner,
    context=[task1, task2]  # Sees both outputs
)
```
<!--  ### Step 5: Gradio Interface
[//]: <>  ```python
[//]: <>  # Why: User-friendly chat interface

[//]: <>  def process_query(query, history):
[//]: <>      result = crew.kickoff()  # Run agents
[//]: <>      return formatted_response

[//]: <>  demo = gr.ChatInterface(
[//]: <>      fn=process_query,
[//]: <>      examples=[...],  # Suggested queries
[//]: <>      share=True       # Public URL
[//]: <>  )
[//]: <>  ```-->

## ğŸ“Š Performance

| Metric | Value |
|--------|-------|
| Query Response Time | 10-30 seconds |
| Document Retrieval | < 1 second |
| LLM Inference | 8-25 seconds |
| GPU Memory Usage | ~4GB |
| Accuracy (subjective) | ~85% |

## ğŸ”’ Privacy & Security

- âœ… All processing happens locally
- âœ… No data sent to external APIs
- âœ… Documents stay on your server
- âœ… GDPR compliant
- âš ï¸ Still verify critical information with official sources

## ğŸ› Troubleshooting

### "ModuleNotFoundError: No module named 'transformers.modeling_layers'"
```bash
pip install --upgrade transformers accelerate
```

### "BadRequestError: LLM Provider NOT provided"
```bash
# Ensure Ollama is running
ollama serve &
ollama list  # Should show phi3
```

### "NumPy/SciPy version conflicts"
```bash
pip uninstall -y numpy scipy
pip install "numpy>=2.0,<2.3" "scipy>=1.13.0"
# Restart runtime
```

### FAISS index not loading
```bash
# Rebuild the index
vectorstore.save_local("/content/data/faiss_index")
```

## ğŸš§ Future Improvements

- [ ] Add conversation memory (multi-turn)
- [ ] Implement streaming responses
- [ ] Add document upload via Gradio
- [ ] Multi-language support (Irish + English)
- [ ] Voice input/output
- [ ] Mobile app version
- [ ] User feedback collection
- [ ] A/B testing different LLMs
- [ ] Automated policy updates

## ğŸ“ License

MIT License - See LICENSE file for details

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

## ğŸ“§ Contact

For questions or support, please open an issue on GitHub.

## ğŸ™ Acknowledgments

- Dublin City Council for public documents
- Anthropic for Claude (used in development)
- Microsoft for Phi-3 model
- Ollama team for inference server
- CrewAI for multi-agent framework
- HuggingFace for embeddings

---

**Built with â¤ï¸ for Dublin citizens**
